\section{Introduction}

%Place notes here for the time being


{In the Natural Language Processing community there is currently an open frontier of research focused on different ways in which texts can be condensed from their original versions into some form of compressed/reduced/summarized output. The general goal of these tasks is to process a text or texts in such a way that important information is preserved but some aspect of complexity is reduced.  Over the course of this paper, I intend to give an overview of three of these tasks: text compression, text simplification, and sentence fusion. While exploring these tasks, I also intend to focus on the use of discourse level information, which, when applicable, represents a promising means of identifying important information in these related tasks.}

{Let me begin by going over some basic definitions of what each task entails.  I'll start with text compression, which according to \citet{Clarke:2010:DCD:1950488.1950493} encompasses automatic methods for shortening sentences with minimal information loss while preserving their grammaticality.  It is important to point out that this definition refers to the process of shortening a text's length on a word by word basis, rather than a sentence by sentence basis, (as is the case with most text summarization systems). In this paper I will present two publications as examples of text compression,  The first is an earlier system from \citet{galley2007lexicalized} which performs compression on isolated sentences without taking surrounding context into account.  The second system is from \citet{Clarke:2010:DCD:1950488.1950493} and performs compression across a whole document using discourse information.  

{Moving next to text simplification, the task can be defined as taking a sentence as input and aiming to produce a simplified version with a less complex vocabulary and/or sentence structure while preserving the main ideas from the original \citep{Feng2008}.  In contrast to text compression, the procedure for simplification is not as clear cut as just removing words (although deletion may play some role). Often the task calls for splitting long sentences into a series of smaller sentences, replacing semantically difficult or ambiguous words with simpler ones, or re-phrasing a sentence to change it's syntactic structure \citep{coster-kauchak:2011:T2TW-2011}.  For this paper I will focus on 2 examples. The first example from from \citet{Siddharthan2006} uses discourse information to break down syntactically complex sentences into smaller sub-sentences.  The second example from \citet{coster-kauchak:2011:T2TW-2011} attempts to reduce the semantic complexity of a text, and frames the simplification task as a translation from standard English to simple English using parallel corpora taken from Wikipedia.\footnote{http://simple.wikipedia.org,	http://en.wikipedia.org}}
 
{Next on to the task of sentence fusion.  Sentence fusion can be defined as creating a summary of a multi-document collection by fusing parts of related sentences together \citep{Filippova:2008:SFV:1613715.1613741}.   The process for doing so is best divided into two steps.  In the first step, often referred to as paraphrase detection, the aim is to search a multi-document database  and identify clusters of equivalent sentences or sentences fragments \citep{Regneri:2012:UDI:2390948.2391048}. In the second step, commonly referred to as the fusion step,  the aim is to create a summary sentence for each paraphrase cluster which maximizes common information across the sentences, while minimizing redundancy.  This is done by combining parts of sentences from a cluster and inserting other function words to ensure a grammatical sentence upon output.\citep{Filippova:2008:SFV:1613715.1613741}. In this paper I will cover an end to end approach of the task by \citet{Filippova:2008:SFV:1613715.1613741}, and a stand alone paraphrase detection system by \citet{Regneri:2012:UDI:2390948.2391048} which makes use of discourse information for better paraphrase clustering.}

{Continuing on from these basic definitions, the remaining sub-sections of the introduction will look in greater detail at the real world applications of these tasks, the models which are used to describe the problem of each task, and the data used for training and testing.  Following the introduction, section 2 will describe in greater detail how each of the systems mentioned above work. This section will also contrast the differences between those system which use discourse information, and those which do not. Section 3 is reserved for discussion of the relations between the tasks, conclusion on the current state of these technologies, and speculation on future work.}


\subsection{Applications}


{As mentioned in the previous section, all three tasks share an over-arching purpose of transforming a text from it's original content into a condensed output. How that condensed output can be put to use is the question I intend to explore in this sub-section.  As one will see, many of the tasks have some overlap in their applications, or they share common goals but vary in their approaches to solving them.  This is indicative of how the tasks are related and lie on a spectrum of different manners in which a text can be reduced.}

{Of the three tasks, compression has the most broad set of applications, so it serves as a good starting point.  Over the course of it's development text compression has proposed as a solution for improving text summarization systems \citep{Jing:2000:SRA:974147.974190} \citep{Knight:2002:SBS:604203.604207} , reducing the length of text for PDA's \citep{corston2001text}, subtitle generation \citep{Clarke:2010:DCD:1950488.1950493}, and as a reading aid for the blind \citep{grefenstette1998producing}. From this diverse range of applications, one can see that compression is used not only to improve information density problems like summarization, but can also be put to good use increasing readability when text length is limited. One of the reasons compression finds so many useful outlets is that the task's purpose of deleting superfluous words can be very easily adapted to so many different domains.  The other two tasks involve more complex operations than just word deletion, and as a result, have a more narrowly defined range of applications at present.}

{







{A good starting point when talking about applications is to look at how text summarization can be augmented by both sentence compression and sentence fusion.  As a baseline, most text summarization systems use extractive approaches which shorten a document by retaining important sentences, and discarding everything else.  While extractive approaches guarantee grammatical output on the sentence level, it's often not a fine-grained enough solution for some summarization goals.  For instance, if the goal is to have a summary with maximum compression of text length, the simple extractive approach falls short.  This is because some texts will inevitably contain sentences with a mix of important and superfluous information.  Sentence compression was envisioned as a means to tackle these cases.  By compressing the sentences which are output from extractive summarization systems, a sentence compression system can further reduce summary length for more concise text summaries.  This is in effect a double compression pipeline which first cuts out un-necessary sentences using extraction, then cuts out un-necessary words using compression.}

{When the goal of a text summarization system is scaled up to summarize a collection of documents,the baseline extractive approach runs into a similar problem of balancing important information with summarization brevity. In the baseline multi-document extractive approach, sentences from across all documents are clustered based on their similarity to each other.  The summary is created by picking sentences from the clusters which are the best representations of important information.  Like in the previous example, sentences are selected whole, which often results is an adequate summary with some superfluous text, or a summary which sacrifices important information in favor of brevity \citep{Filippova:2008:SFV:1613715.1613741}. In these cases sentence fusion was envisioned as a means to pick the best snippets of informative text from a cluster and fuse them into new sentence, thus greatly reducing the amount of redundant or unneeded text.\citet{bla}.}

{While text summarization plays a large role in the development of both compression and sentence fusion, there are many other applications which also call for the use of compression based tasks.  Transforming texts for greater readability has also been a popular topic of research, and has made good use of text compression and text simplification.  In the case of text simplification, the goal of improving readability is often focused on people with reading comprehension difficulties such as children and foreign language learners, or those with cognitive impairments such as aphasics\citep{Feng2008}. In such applications, the purpose of text simplification is to reduce the semantic complexity of a text, and/or to alleviate some memory load issues by breaking complex sentences down into shorter, more easily processed sentences.  Some other readability applications include using text compression to reducing text length for display on PDA's \citep{Corston-Oliver 2001} or as a reading aid for the blind \citep{Grefenstette 1998}.  In these cases, compression will suffice because it's readability issues arise from text length rather than text difficulty.}
 
 
\iffalse
{Text simplification has also generated interest in the field of information extraction as a means to improve recall for some specialized document collections.  For instance, in the bio-medical field, processing documents using the standard NLP approaches of POS taggers, parse trees, and dependency trees results in poor performance due to the syntactic complexity of academic writing.  By breaking up these long syntactically complex sentences into smaller sentences using text simplification, recall is improved \citep{jonnalagadda2010biosimplify}.}
\fi


\subsection{Models}
{This subsection is intended to describe how the problem of each task is modeled.  By looking at what operations are to be performed, and the constraints governing those operations, one can see how the three tasks are related.} 

{Beginning with compression, the task is typically viewed as a word deletion operation.  According to \citet{Knight:2002:SBS:604203.604207}, given an input sentence of words $x= x_{1},x_{2}...x_{n}$ the aim is to produce a compression which is some sub-set of these words.  The constraints in this model can be viewed conceptually as the rules which decide which words are kept, and which are deleted.}


{The approach used to solve the problem of text simplification expands upon the deletion problem mentioned in the previous paragraph.  In addition to the deletion operation, the operations of reduction, rewording, reordering, and insertion may also be included in modeling the task\citep{Siddharthan2006}\citep{coster-kauchak:2011:T2TW-2011}.  In terms of constraints which determine what operation to perform, the example system from \citet{Siddharthan2006} uses hand coded rules based on syntatic information, while the system from \citet{coster-kauchak:2011:T2TW-2011} is a data driven approach which learns rules for the operations from parallel corpora or standard and simple texts.}

{The approaches used for sentence fusion must be viewed as two sub tasks.  The task of paraphrase detection uses a 


\subsection{Data}

{Highlight the kind of data used to train and/or model the problem for each text reduction task. Is the data directly used to train a system, or is it simply used as a frame of reference for un-supervised learning.  What kind of data is used for validation and evaluating the systems? }


{This section will focus on the data used to train





\subsection{Natbib citations}
Within a text, you can say that \citet{lin2001} found out something. Or you can just state the thing, and then put the author in parentheses \citep[see][]{szpektor2004}.