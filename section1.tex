\section{Introduction}

%Place notes here for the time being
\iffalse
{Talk about what the three different categories are, and how they can be differentiated from each other.  (this should be about 2 paragraphs).  
Then talk about what commonalities each category has with the others(this can be again 1-2 paragraphs) }
\fi


{In the Natural Language Processing community there is currently an open frontier of research tasks focused on ways in which texts can be compressed or transformed from their original versions into some form of distilled output. The general goal of these tasks is to reduce a single text or several texts in such a way that important information is preserved but some aspect of complexity is reduced.  Over the course of this paper, I intend to give an overview of three of these tasks: text compression, text simplification, and sentence fusion. While exploring these tasks, I also intend to focus on the use of discourse level information, which, when applicable, represents a promising means of identifying important information in these compression related tasks.}

{Let me begin by providing some basic definitions of what each task entails, and pointing out which publications this paper will focus on. I'll begin with text compression as it is the most straightforward definition and will serve as a reference point for how the other two tasks diverge. Text compression was originally defined as a sentence level operation intended to reduce the length of a sentence (or produce a sentence summary) which preserves the most important information while also remaining grammatical \citet{Jing2000}.  In order to provide more flexible compression of whole documents, recent systems have been developed which used discourse information across the whole document, rather than just on the sentence level, in order to make more informed choices about what words get compressed.  In this paper, I will be focusing my attention on two examples of compression by Clarke \& Lapata which show the transition from sentence level information\citet{Clarke,Lapata2008}  to discourse level\citet{Clarke,Lapata2010}.}

{The task of text simplification can be defined as: given a source sentence we aim to produce a simplified version of that sentence with simpler vocabulary and sentence structure while preserving the main ideas in the original sentence\citet{Feng2008}.  In contrast to text compression, the procedure for simplification is not as clear cut as just deleting/retaining words (although deletion may play some role). Often the task calls for splitting long sentences into a series of smaller sentences, replacing semantically difficult or ambiguous words with simpler ones, or re-phrasing a sentence to change it's syntactic structure.\citet{Coster,Kauchak2011}.  For this paper I will focus on 2 examples. The first example from from \citet{Siddharthan2006} uses discourse information to break down syntactically complex sentences into smaller sub-sentences.  The second example from \citet{Coster,Kauchak2011} attempts to reduce the semantic complexity of a text, and frames the simplification task as a translation from standard to simple English using parallel corpora taken from Wikipedia (need to add like citation).}
 
{The task of sentence fusion may be seen as somewhat similar to text compression, but applied over a multi-document collection of texts.  However, to adequately describe the task of sentence fusion, it is best to divide it into a two step process. The first step is a pre-processing step of paraphrase detection, and the second step is the sentence fusion itself.  In paraphrase detection, the aim is to search a multi-document database  and identify clusters of equivalent sentences or sentences fragments \citet{Regneri,Wang2012}. In the sentence fusion step, the aim is to create a summary sentence for each paraphrase cluster which maximizes common information across documents, while minimizing redundancy.  This is done by fusing parts of sentences from the cluster together to make one concise, cohesive sentence.\citet{Filippova, Strube2008}.  Using the two articles cited above as my examples, I will demonstrate how both steps in the sentence fusion process can benefit from the use of discourse information.}

{Continuing on from these basic definitions, the remaining sub-sections of the introduction will look in greater detail at the real world applications, the models which are used to tackle each task, and the data for training and testing.  Following the introduction, section 2 will detail the systems from each of the publication mentioned above, and how they use discourse information (if applicable). Section 3 is reserved for discussion of the relations between compression based tasks, conclusion on the current state of these technologies, and speculation on future work.}


\subsection{Applications}


{As mentioned in the previous section, all three tasks being discussed share an over-arching purpose of transforming a text from it's original content into a more manageable and useful sequence of words. Naturally, what that new sequence of words should look like is determined by the end goal of the application. In this sub-section we are interested in those end goals, and what kinds of applications the three tasks have been developed to handle.}

{A good starting point when talking about applications is to look at how text summarization precipitated the need for both sentence compression and sentence fusion.  As a baseline, most text summarization systems use extractive approaches which shorten a document by retaining important sentences, and discarding everything else.  While extractive approaches guarantee grammatical output on the sentence level, it's often not a fine-grained enough solution for some summarization goals.  For instance, if the goal is to have a summary with maximum compression of text length, the simple extractive approach falls short.  This is because some texts will inevitably contain sentences with a mix of important and superfluous information.  The only option in an extractive system is to discard the whole sentence, which sacrifices information, or to include the whole sentence, which makes the summary unnecessarily long.  Sentence compression was originally envisioned as a means to tackle these cases.  By compressing the sentences which are output from extractive summarization systems, a sentence compression system can further reduce summary length for more concise text summaries\citet{Jing2000}.}

{When the goal of a text summarization system shifts from summarizing a single document to a collection of documents, it runs into a similar problem of balancing the inclusion of important information with the need for brevity.  For instance, in a standard extractive approach to multi-document summarization, sentences for the summary are picked based on how well they represent information common to all the texts in document collection \citet{bla}.The problem, again, is that the selected sentences may also contain extraneous information. Often the result is an adequate summary of the important information with some amount of repetitive or superfluous text, or a summary which sacrifices some important information in favor of brevity. In these cases sentence fusion was envisioned as a means to pick and choose the best snippets of informative text from the collection, without being limited by a whole sentences as the unit of selection\citet{bla}.  This means that phrases or clauses from several different sentences can be fused together abstractively rather than extractively to make a more concise summary of the document collection.}

{While text summarization plays a large role in the development of both compression and sentence fusion, there are many other applications which also call for the use of compression based tasks.  Transforming texts for greater readability has also been a popular topic of research, and has made good use of text compression and text simplification.  In the case of text simplification, the goal of improving readability is often focused on people with reading comprehension difficulties such as children and foreign language learners, or those with reading impairments such as aphasics\citet{Feng2008}. In such applications, the purpose of text simplification is to reduce the semantic complexity of a text, and/or to alleviate some memory load issues by breaking complex sentences down into shorter, more easily processed sentences.  Some other readability applications include using text compression to reducing text length for display on PDA's \citep{Corston-Oliver 2001} or as a reading aid for the blind \citep{Grefenstette 1998}.}
 
{ Write another paragraph about improving information retreival as well}

\subsection{Approaches}
{In this sub-section I will discuss the approaches used to model the tasks of compression, simplification and fusion.  As I will point out in the following paragraphs, the models for describing each task share many common concepts, which is helpful in describing how the tasks are related.  } 

{Beginning with compression, the task is typically viewed as a word deletion problem governed by a set of constraints.  According to Knight and Marcau\citet{KnightMarcau2002}, given an input sentence of words $x= x_{1},x_{2}...x_{n}$ the aim is to produce a compression which is some sub-set of these words.  The constraints for selecting the subset of words can be learned as a probabilistic context free grammar trained on  parallel corpus of original and compressed sentences\citep{KnightMarcau2002}. Alternatively it can be based on syntactic information, such as learning which constituents to delete from a parse tree \citep{ClarkLapata 2008}.  While the problem is modeled as a sentence level operation, modern approaches such as the one I will be covering from the Clark and Lapata \citep{ClarkLapata 2010} paper show that discourse information can also be included in developing constraints for sub-set selection.}

{The approach used to solve the problem of text simplification expands upon the deletion problem mentioned in the previous paragraph.  In addition to the deletion operation, the operations of reduction, rewording, reordering, and insertion may also be included in modeling the task\citep{SIDDHARTHAN2006}\citep{Kauchak2011}.  In terms of constraints which determine what operation to perform, the example system from \citet{SIDDHARTHAN2006} uses hand coded rules based on syntatic information, while the system from \citet{Kauchak2011} is a data driven approach which learns rules for the operations from parallel corpora or standard and simple texts.}

{The approach 


\subsection{Data}

{Highlight the kind of data used to train and/or model the problem for each text reduction task. Is the data directly used to train a system, or is it simply used as a frame of reference for un-supervised learning.  What kind of data is used for validation and evaluating the systems? }





\subsection{Natbib citations}
Within a text, you can say that \cite{lin2001} found out something. Or you can just state the thing, and then put the author in parentheses \citep[see][]{szpektor2004}.