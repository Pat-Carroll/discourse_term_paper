\section{Introduction}

%Place notes here for the time being


{In the Natural Language Processing community there is currently an open frontier of research focused on different ways in which texts can be condensed from their original versions into some form of compressed/reduced/summarized output. The general goal of these tasks is to process a text or texts in such a way that important information is preserved but some aspect of complexity is reduced.  Over the course of this paper, I intend to give an overview of three of these tasks: text compression, text simplification, and sentence fusion. While exploring these tasks, I also intend to focus on the use of discourse level information, which, when applicable, represents a promising means of identifying important information in these compression related tasks.}

{Let me begin by going over some basic definitions of what each task entails.  I'll start with text compression, which according to \citet{Clarke:2010:DCD:1950488.1950493} encompasses automatic methods for shortening sentences with minimal information loss while preserving their grammaticality.  It is important to point out that this definition refers to the process of shortening a text's length on a word by word basis, rather than a sentence by sentence basis, (as is the case with most text summarization systems). In this paper I will present two publications as examples of text compression, and some of the recent development in adding discourse information.  The first is an earlier system from \citet{} which performs compression on isolated sentences without taking surrounding context into account.  The second system is from \citet{Clarke:2010:DCD:1950488.1950493} and performs compression across a whole document using discourse information.  

{Moving next to text simplification, the task can be defined as taking a sentence as input and aiming to produce a simplified version of that sentence with a more basic vocabulary and/or sentence structure while preserving the main ideas from the original \citet{Feng2008}.  In contrast to text compression, the procedure for simplification is not as clear cut as just removing words (although deletion may play some role). Often the task calls for splitting long sentences into a series of smaller sentences, replacing semantically difficult or ambiguous words with simpler ones, or re-phrasing a sentence to change it's syntactic structure \citep{coster-kauchak:2011:T2TW-2011}.  For this paper I will focus on 2 examples. The first example from from \citet{Siddharthan2006} uses discourse information to break down syntactically complex sentences into smaller sub-sentences.  The second example from \citet{coster-kauchak:2011:T2TW-2011} attempts to reduce the semantic complexity of a text, and frames the simplification task as a translation from standard English to simple English using parallel corpora taken from Wikipedia.\footnote{http://simple.wikipedia.org,	http://en.wikipedia.org}}
 
{Now for the The task of sentence fusion. may be seen as somewhat similar to text compression, but applied over a multi-document collection of texts.  However, to adequately describe the task of sentence fusion, it is best to divide it into a two step process. The first step is a pre-processing step of paraphrase detection, and the second step is the sentence fusion itself.  In paraphrase detection, the aim is to search a multi-document database  and identify clusters of equivalent sentences or sentences fragments \citep{Regneri:2012:UDI:2390948.2391048}. In the sentence fusion step, the aim is to create a summary sentence for each paraphrase cluster which maximizes common information across documents, while minimizing redundancy.  This is done by fusing parts of sentences from the cluster together to make one concise, cohesive sentence.\citep{Filippova:2008:SFV:1613715.1613741}.  Using the two articles cited above as my examples, I will demonstrate how both steps in the sentence fusion process can benefit from the use of discourse information.}

{Continuing on from these basic definitions, the remaining sub-sections of the introduction will look in greater detail at the real world applications, the models which are used to tackle each task, and the data for training and testing.  Following the introduction, section 2 will detail the systems from each of the publication mentioned above, and how they use discourse information (if applicable). Section 3 is reserved for discussion of the relations between compression based tasks, conclusion on the current state of these technologies, and speculation on future work.}


\subsection{Applications}


{As mentioned in the previous section, all three tasks share an over-arching purpose of transforming a text from it's original content into a condensed form. Just what that condensed text can be useful for is the purpose of this sub-section. Here we will look at what applications the tasks have been developed for, and why they may benefit that .}

{A good starting point when talking about applications is to look at how text summarization can be augmented by both sentence compression and sentence fusion.  As a baseline, most text summarization systems use extractive approaches which shorten a document by retaining important sentences, and discarding everything else.  While extractive approaches guarantee grammatical output on the sentence level, it's often not a fine-grained enough solution for some summarization goals.  For instance, if the goal is to have a summary with maximum compression of text length, the simple extractive approach falls short.  This is because some texts will inevitably contain sentences with a mix of important and superfluous information.  Sentence compression was envisioned as a means to tackle these cases.  By compressing the sentences which are output from extractive summarization systems, a sentence compression system can further reduce summary length for more concise text summaries\citep{Jing:2000:SRA:974147.974190}.  This is in effect a double compression pipeline which first cuts out un-necessary sentences using extraction, then cuts out un-necessary words using compression.}

{When the goal of a text summarization system is scaled up to summarize a collection of documents,the baseline extractive approach runs into a similar problem of balancing important information with summarization brevity. In the baseline multi-document extractive approach, sentences from across all documents are clustered based on their similarity to each other.  The summary is created by picking sentences from the clusters which are the best representations of important information.  Like in the previous example, sentences are selected whole, which often results is an adequate summary with some superfluous text, or a summary which sacrifices important information in favor of brevity \citep{Filippova:2008:SFV:1613715.1613741}. In these cases sentence fusion was envisioned as a means to pick the best snippets of informative text from a cluster and fuse them into new sentence, thus greatly reducing the amount of redundant or unneeded text.\citet{bla}.}

{While text summarization plays a large role in the development of both compression and sentence fusion, there are many other applications which also call for the use of compression based tasks.  Transforming texts for greater readability has also been a popular topic of research, and has made good use of text compression and text simplification.  In the case of text simplification, the goal of improving readability is often focused on people with reading comprehension difficulties such as children and foreign language learners, or those with cognitive impairments such as aphasics\citep{Feng2008}. In such applications, the purpose of text simplification is to reduce the semantic complexity of a text, and/or to alleviate some memory load issues by breaking complex sentences down into shorter, more easily processed sentences.  Some other readability applications include using text compression to reducing text length for display on PDA's \citep{Corston-Oliver 2001} or as a reading aid for the blind \citep{Grefenstette 1998}.  In these cases, compression will suffice because it's readability issues arise from text length rather than text difficulty.}
 
 
\iffalse
{Text simplification has also generated interest in the field of information extraction as a means to improve recall for some specialized document collections.  For instance, in the bio-medical field, processing documents using the standard NLP approaches of POS taggers, parse trees, and dependency trees results in poor performance due to the syntactic complexity of academic writing.  By breaking up these long syntactically complex sentences into smaller sentences using text simplification, recall is improved \citep{jonnalagadda2010biosimplify}.}
\fi


\subsection{Models}
{This subsection is intended to describe how the problem of each task is modeled.  By looking at what operations are to be performed, and the constraints governing those operations, one can see how the three tasks are related.} 

{Beginning with compression, the task is typically viewed as a word deletion operation.  According to \citet{Knight:2002:SBS:604203.604207}, given an input sentence of words $x= x_{1},x_{2}...x_{n}$ the aim is to produce a compression which is some sub-set of these words.  The constraints in this model can be viewed conceptually as the rules which decide which words are kept, and which are deleted.}


{The approach used to solve the problem of text simplification expands upon the deletion problem mentioned in the previous paragraph.  In addition to the deletion operation, the operations of reduction, rewording, reordering, and insertion may also be included in modeling the task\citep{Siddharthan2006}\citep{coster-kauchak:2011:T2TW-2011}.  In terms of constraints which determine what operation to perform, the example system from \citet{Siddharthan2006} uses hand coded rules based on syntatic information, while the system from \citet{coster-kauchak:2011:T2TW-2011} is a data driven approach which learns rules for the operations from parallel corpora or standard and simple texts.}

{The approaches used for sentence fusion must be viewed as two sub tasks.  The task of paraphrase detection uses a 


\subsection{Data}

{Highlight the kind of data used to train and/or model the problem for each text reduction task. Is the data directly used to train a system, or is it simply used as a frame of reference for un-supervised learning.  What kind of data is used for validation and evaluating the systems? }


{This section will focus on the data used to train





\subsection{Natbib citations}
Within a text, you can say that \citet{lin2001} found out something. Or you can just state the thing, and then put the author in parentheses \citep[see][]{szpektor2004}.