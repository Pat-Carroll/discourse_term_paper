\section{Introduction}

%Place notes here for the time being


{In the Natural Language Processing community there is currently an open frontier of research focused on different ways in which a text can be condensed from it's original versions into some form of compressed/simplified/summarized output. The general goal of these tasks is to process a text in such a way that important information is preserved but some aspect of complexity is reduced.  Over the course of this paper, I intend to present and compare two closely related tasks in this field: text compression and  text simplification. While exploring these tasks, I also intend to focus on the use of discourse level information which, when applicable, represents a promising new avenue of research for both text compression and text simplification tasks.}

{Let me begin by going over some basic definitions of what each task entails.  I'll start with text compression, which according to \citet{Clarke:2010:DCD:1950488.1950493} encompasses automatic methods for shortening sentences with minimal information loss while preserving their grammaticality.  It is important to point out that this definition refers to the process of shortening a text's length on a word by word basis, rather than a sentence by sentence basis, (as is the case with most text summarization systems). In this paper I will present three publications as examples of text compression,  The first two papers from \citet{knight2000statistics} and \citet{galley2007lexicalized} describe noisy channel based approaches  which performs compression on isolated sentences without taking surrounding context into account.  The third system is from \citet{Clarke:2010:DCD:1950488.1950493} and performs compression across a whole document using discourse information.  

{For text simplification, the task can be defined as taking a sentence as input and aiming to produce a simplified version with a less complex vocabulary and/or sentence structure while preserving the main ideas from the original text \citep{Feng2008}.  In contrast to text compression, the procedure for simplification is not as clear cut as just removing words (although deletion may play some role). Often the task calls for splitting long sentences into a series of smaller sentences, replacing semantically difficult or ambiguous words with simpler ones, or re-phrasing a sentence to change it's syntactic structure \citep{coster-kauchak:2011:T2TW-2011}.  For this paper I will focus on 2 examples. The first example from from \citet{Siddharthan2006} uses discourse information to break down syntactically complex sentences into smaller sub-sentences.  The second example from \citet{coster-kauchak:2011:T2TW-2011} attempts to reduce the semantic complexity of a text, and frames the simplification task as a translation from standard English to simple English using parallel corpora taken from Wikipedia.\footnote{http://simple.wikipedia.org,	http://en.wikipedia.org}}


{Continuing on from these basic definitions, the following sub-sections of the introduction will discuss some of the the real world applications motivating these tasks, the models which are used to describe the problem of each task, and the data used for training and testing.  Following the introduction, section 2 will describe in greater detail how each of the systems work, and how they relate to one-another. Section 3 is reserved for general discussion on the use of discourse information in the tasks, conclusion on the current state of these technologies, and speculation on future work.}


\subsection{Applications}


{As mentioned in the previous section, both tasks share an over-arching purpose of transforming a text from it's original content into a condensed output. How that condensed output can be put to use is the question being explored in this sub-section.  In many cases, the applications for text compression have a certain amount of overlap with text simplification an vice-versa. One can view this as indicative of how closely the tasks are related and how they  lie on a spectrum of different ways in which text complexity is reduced.}

{Of the two tasks, compression has the most broad set of applications.  Over the course of it's development text compression has been proposed as a post processing step for improving text summarization systems \citep{Jing:2000:SRA:974147.974190,Knight:2002:SBS:604203.604207} , as a means of reducing text length for PDA's \citep{corston2001text}, as a component of subtitle generation \citep{vandeghinste2004sentence}, and as a reading aid for the blind \citep{grefenstette1998producing}. From this diverse range of applications, one can see that compression is used not only to improve information density problems like summarization, but can also be put to  use increasing readability when text length is limited. One of the reasons compression finds so many useful outlets is that the task's purpose of deleting superfluous words is a relatively broad tool which can be very easily adapted to many different domains.  Because the task of simplification involves several different edit operations, it appears to have more narrowly defined range of applications at present.}


{While transforming texts for greater readability can be achieved by text compression (as mentioned earlier for subtitles and PDAs), text simplification is a tool specifically envisioned for such situations.  Where it differs from compression is that, by reducing semantic or syntatic complexity, rather than just length, it can be a more targeted approach to increasing readability for specific groups of people.  For instance it has been proposed as a tool to help people with reading comprehension difficulties such as children and foreign language learners, or those with cognitive impairments such as aphasics\citep{Feng2008}. In such applications, psychological and pedagogic theories are taken into account in order to try and reduce memory load and cognitive processing for readers.  Text simplification has also generated interest in the fields of parsing,machine translation, and information extraction as a pre-processing step. In theses cases, simplification is used to break up large, syntactically complex sentences into smaller sentences which is expected to boost performance   further down the pipeline \citep{jonnalagadda2010biosimplify,Siddharthan2006}.}


\subsection{Models}
{This subsection is intended to describe how the problem of each task is modeled.  Beginning with compression, the task is typically viewed as a word deletion operation.  According to \citet{Knight:2002:SBS:604203.604207}, given an input sentence of words $x= x_{1},x_{2}...x_{n}$ the aim is to produce a compression which is some sub-set of these words. The rules for how that sub-set of words is selected will differ for each specific system discussed.   In the first two papers  \citep{galley2007lexicalized,knight2000statistics}  the sub-set of words is found using translation rules which describe a transformation from a source text to it's most likely compression. In the third paper \citep{Clarke:2010:DCD:1950488.1950493}, the optimal sub-set is selected using a scoring function which evaluates candidate compressions against a set of constraints that enforce compression rate, penalize ungrammaticality, and encourage retention of discourse relevant words.}


{The general model for the problem of text simplification expands upon the deletion problem mentioned in the previous paragraph.  In addition to the deletion operation, the operations of word substitution, reordering, insertion, and sentence splitting may also be included in modeling the task \citep{Siddharthan2006,coster-kauchak:2011:T2TW-2011}.  Most simplification system only use \textit{some} of the operations mentioned, and so it's important to note that different simplification systems vary widely in their final output.   In the first simplification paper from \citet{Siddharthan2006} the authors use hand coded rules based on discourse information to split long sentences  into smaller sub-sentences.  In a second step they address issues of cohesion by performing re-ordering operations and insertions.   The second simplification system from \citet{coster-kauchak:2011:T2TW-2011} is more focused on semantic simplification and performs operations of deletion, substitution, re-ordering, and insertion on the text. The transformation from standard to simplified text is modeled as phrase based translation, somewhat similar to the translation model in the compression papers.}


\iffalse
\subsection{Data}

{The data used to train each system is diverse and therefore not suitable for direct comparison of results.  Nonetheless, it's helpful to see what each system is working with, in order to get a sense of some of the advantages and challenges each data set carries with it. The first two translation based compression systems from \citet{knight2000statistics} and \citet{galley2007lexicalized} are trained on the Zipf Davis Corpus of article/abstract pairs.  The challenge for both systems using this corpus is obtaining sufficient source/compression sentence pairs for reliable translation rule probabilities.  the later system by \citet{galley2007lexicalized} takes steps to address this, but the training data remains on the order of thousands of source / compression sentence pairs.}

{The training data used for the discourse based compression by \citet{Clarke:2010:DCD:1950488.1950493} acknowledges the prior work done with the Zipf Davis corpus, but chooses a hand made corpus 

{Highlight the kind of data used to train and/or model the problem for each text reduction task. Is the data directly used to train a system, or is it simply used as a frame of reference for un-supervised learning.  What kind of data is used for validation and evaluating the systems? }

\fi
